# 李宏毅-机器学习课程 2021 版-学习笔记


## Machine Learning

Machine Learning ≈ 确定某个特殊的Function（函数），其具有特定的输入和输出

### Function的类别

**Regression**
Regression: 输入一个序列，输出一个标量（Scalar）

**Classification**
Classification：输入类别（Class）或选项（Option），输出正确的选项或类别

Alpha Go，就是一种Classification，其中class就是棋盘的位置

**Structure Learning**
Structure Learning：输出的内容是结构化的，如图片、文档等


### 确定Function的步骤

#### Define Model

**第一步**：确定带有未知参数的Function，即**确定Model**

基于Domain Knowledge，推测出带参Function的函数式，即推测Model。如一元二次函数、三元四次函数等，线性Model，或者多项式回归、神经网络等非线性Model

其中已知的先验数据集中，**feature**代表数据的数值，**label**则代表数据的分类

#### Define Loss Function

**第二步**：基于Training Data和Model，确定**Loss Function**

Loss Function是基于Training Data，构建出的函数，用于判断Model参数的好坏

Loss Function其输入是Model参数的一组数值，输出的标量，即**误差error或loss**，代表着Model参数的好坏

Loss Function中常常会基于MAE（mean absolute error）、MSE（mean square error）等方式，来计算误差error

以Loss Function的一组输入参数为超平面（hyperplane）坐标，可以是任意（维度）数量的，以error的值作为此超平面的法向量的长度，进而构成的多维空间中的曲面，被称为**误差曲面（error surface）**

#### Optimization

**第三步**：最优化**Optimization**，即**确定最优Parameter**

最优Parameter：使得Loss值最小的Parameter。

每次更新Parameter的过程，被称为update。

Gradient Descent（梯度下降）方法：
随机选取一个起始点，然后通过计算Loss Function中误差曲面（error surface）的微分（即二维的斜率），以便朝着Loss下降最快的方向调整参数，进而逐渐确定Loss最小值。

机器学习过程中，需要自己手动设置的参数，如learning rate等，被称为**Hyperparameter**。
PS：learning rate控制的是每次Parameter在update时的其变化的跨度。



## Deep Learning

深度学习是一种以人工神经网络为基础的机器学习方法，属于机器学习的一个分支，一个具体的实现方法。

其中人工神经网络就是机器学习中的一种Model，神经网络中的每个神经元（Neuron）可以看做是一个函数（Function），一组神经元组成一个层次（Layer），而多个层次共同组成一个神经网络（Neural Network）。

深度学习中的深度（Deep），代表着神经网络模型中的层数较多，即网络中包含多个隐含层的情况。

Function => Neuron

Neurons => Layer

Layers => Neural Network

Deep = Many hidden layers（隐含层）

每个隐含层的输出，是下一个隐含层的输入


## Model常见问题及诊断技巧

先检查Training Data Loss，如果Training Data Loss很大，则可能是Model Bias，或者是Optimization的问题。

如果Training Data Loss很小，则检查Testing Data Loss。

如果Testing Data Loss很大，则可能是Overfitting，或者Mismatch的问题。

反之，如果Testing Data Loss和Testing Data Loss都很小，则代表Model很好。


### Model Bias

如果Training Loss结果太大，则可能是Model Bias的问题。

通常情况下，是因为Model过于简单，导致拟合效果太差，预测结果与真实结果相差太大。

常见的解决方案是重新设计Model，采取更加复杂的Model，如：增加更多的Layer等。



### Optimization Issue

可以通过对比模型，来诊断是否是Optimization环节的问题：层次更多的Model，但是Loss却大于层次更少的Model，可以推断是Optimization的问题。

1. 使用层次少的网络构建Model，或者其他能够容易进行optimization的Model，用于后续对照
2. 使用层次多的网络构建Model，如果在相同Training Data的前提下，Deeper Network Model相比于层次少的Model的Loss更高，则可以推断是Optimization的问题

常用解决方案：
1. 更换Optimization方法，以选取确定更好的Parameter


### Overfitting

在机器学习中，Overfitting（过拟合）是指模型在训练集上表现得很好，但在新数据（测试集或实际应用中的数据）上表现较差的现象。当模型过度拟合训练数据时，它在训练数据上学习到了训练集的细节和噪声，导致在未见过的数据上泛化能力较差。

Overfitting 通常发生在模型具有较高的复杂性或参数数量时，使得模型过于灵活以适应训练数据中的细微变化。这种过度拟合的行为导致模型对训练集中的噪声和异常值过度敏感，而忽略了数据背后的真实模式和普遍规律。

可以通过对比 Training Data 和 Testing Data 下的 Loss，如果 Training Data 下的 Loss 结果，远小于 Testing Data，则可以推断是 Overfitting 的问题。

常用解决方案：
1. 增大Training Data
	1. Data augmentation，如图片放大缩小、左右反转等
	2. 采集新数据
2. 降低Model的灵活度和复杂度
	1. Less parameter, share parameters
	2. Less features
	3. Early stopping
	4. Regularization
	5. Dropout

Training Data的Loss一般会随着Model复杂度的提升，而不断减小，但Testing Data的Loss会先随着Model复杂度的提升而减小，当突破某阈值之后，便会随着Model复杂度的提升而增加，进而出现Overfitting的问题，即Testing Loss要远大于Training Loss。


### Mismatch

Mismatch指的是Training Set和Testing Set在分布上的区别太大，进而使得Testing Loss数值明显大于Training Loss。


### 如何选出最好的Model

Cross Validation

将Training Set分成Training Set和Validation Set，使用Training Set训练Model完成之后，可以使用Validation Set去验证Model，相当于提前验证Model。

避免最后在应用Testing Set时结果不如意。


## Training Batch

Training Set被切分为的子集被称为Batch，而一次Batch的训练过程就是一次update过程，将所有Batch训练过一次的过程被称为一个epoch。

Batch优化方法指的是，将Batch作为Training的基本单位，每次都会计算Loss，以及进行参数的update。

### Small Batch vs Large Batch

1. update速度：在串行运算的情况下，Small Batch更快；在并行运算的情况下，Small Batch和Large Batch差不多。
2. epoch耗时：Small Batch耗时更多，Large Batch耗时更少。
3. Gradient分布：Small Batch下Gradient分布更随机，而Large Batch更集中。
4. Optimization效果：在Optimization阶段，Small Batch优于Large Batch
5. Generalization(Testing)效果：在Testing阶段，即使用新数据测试时，Small Batch训练结果的表现，要优于Large Batch


## Optimization Issue诊断技巧

### 常见的issue

Optimization阶段，即便不断尝试使用新的参数进行训练，但模型的Loss依旧很高，且不怎么变化，那就很可能是遇见了critical point问题，即Gradient值接近于0的情况。

### critical point

critical point问题主要可以分为，local minima和saddle point两大类。

通过计算Hessian矩阵的结果，并结合其eigenvalue，便可以判断当前遇到的critical point问题到底是local minima，还是saddle point的问题。

local minima，相比于saddle point，实际上并没有那么常见。


## Optimizer

在机器学习中，Optimizer（优化器）是指，在Optimization阶段，用于优化模型参数的算法或方法。机器学习的目标通常是通过最小化（或最大化）一个损失函数值来找到最优的模型参数，而优化器则是负责调整模型参数以实现损失函数最小化（或最大化）的工具。

### Momentum Optimizer

Momentum是一种常用的优化算法（Optimizer），用于在Optimization过程中，**调整Gradient的下降方向**，加速Gradient Descent方法的收敛速度，并且可以减少出现Saddle Point和Local Minima问题的可能。

Momentum算法中，Parameter的update方向，不再一味的基于通用Gradient Descent算法的计算结果，而是在此基础上，引入了之前历史的update方向，来作为新方向的计算基础，这种新算法常被称为“Gradient Descent With Momentum”。

### Learning  Rate Optimizer

#### Adaptive Learning  Rate

一般情况下，训练Model时，如果遇到update的次数不断增多，但Loss变化不大的情况时，很容易让人联想到是不是遇见了Critical Point问题，但实际上也有可能是Learning  Rate这种Hyperparameter的值设置不当导致的，即可能Parameter的update step太小（**Gradient的下降速度太慢**），亦或是太大（**Gradient的下降速度太快**）。

理论上，为了保证Optimization的质量，通常需要根据不同的Parameter组，来设置不同的Learning  Rate，因此动态的Learning  Rate就很有必要。

##### Adagrad

Adagrad (Adaptive Gradient)通过计算Gradient的Root Mean Square值，来确定下一次update过程的Learning  Rate参数。

使用这种方式，当Gradient数值较大时，则会降低Learning Rate，反之则会增大Learning Rate，故可以确保稳定的step size进行parameter update。


##### RMS Prop

在Adagrad的Root Mean Square计算公式中，给Gradient增加了可自定义的权重，即Gradient不再是个个平等，而是有轻重之分。

##### Adam

Adam（Adaptive Moment Estimation），即RMS Prop + Momentum。


#### Learning Rate Scheduling

Learning Rate Scheduling，即基于时间来设置不同的Learning Rate。

##### Learning Rate Decay

在Learning Rate Decay方法中，Learning Rate值随着训练时间的增加，而不断衰减，此算法可以使得Loss在前期保持一个较高的衰减速率，而到了训练后期，可以平滑地收敛到某一水平，而不会产生较大的震荡。

##### Warm Up

在Warm Up中，Learning Rate会先变大，后变小，在训练BERT模型时，常常需要使用的Warm Up方法。


## Classification
https://www.youtube.com/watch?v=O2VkP8dJ5FE&list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&index=7


## Batch Normalization
https://www.youtube.com/watch?v=BABPWOkSbLE&list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&index=8


## Neural Network (NN)
https://www.youtube.com/watch?v=OP5HcXJg2Aw&list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&index=9


### Fully Connected Neural Network (FCNN)

FCNN 是一种基本的神经网络模型，由多个全连接层 (Fully Connected Layer) 组成。每个全连接层中的每个神经元都与前一层的所有神经元相连，每个连接都有相应的权重。这意味着每个神经元接收来自前一层所有神经元的输入，并生成输出。

FCNN 中的每层全连接层 (Fully Connected Layer) 中的每一个 Neuron 针对输入的高维向量中每一个维度的分量都需要有 Weight 参数，如一层 Network 有 1000 个 Neuron，而输入的向量维度为 $100*100*3$，则 Model 一共有 $100*100*3*1000=3*10^7$ 个 Weight 参数。

FCNN 在图像识别、文本分类、语音识别等各种机器学习任务中都被广泛应用。它是一种通用的模型，能够学习和表示复杂的非线性关系。

值得注意的是，FCNN 是一种基础的神经网络模型，其仅由全连接层组成，缺乏一些特定的结构，如卷积层和池化层。对于特定的任务和数据类型，可以根据需要对 FCNN 进行调整或与其他类型的层结合使用，以构建更复杂和强大的模型。

### Convolutional Neural Network (CNN)

CNN (Convolutional Neural Network) Model 主要用于图像分类任务，用于将输入的图像分为不同的类别，属于监督和深度学习的一种，是专门用于影像处理的 Neural Network。

#### Image Classification

在 Image Classification 场景中 CNN 的输入输出都是向量。

输出向量的 Dimention 长度决定了模型能够识别的对象的种类，Model 的输出向量和真实答案向量之间的 Cross entropy 越小，则代表模型识别的精度越高。

大于二维的矩阵，被称为张量（Tensor），而每一张图片，一般都是通过三维的 Tensor 来进行存储（即三维数组），这三个维度分别是长度、宽度、通道（channel），而其中的每一个数值，都代表某个二维位置上的某个通道的颜色数值大小，即这个三维立方体中某个位置上存储的颜色数值大小，而元素的角标，则代表其在对应维度的位置。在图像分类的 Model 中，一般输入 Network 的向量，是由张量中各个分量拼接而成的高维向量。

如：
```json
[
	[
		[5, 78, 2, 34, 0],
		[6, 79, 3, 35, 1],
		[7, 80, 4, 36, 2]
	],
	[
		[5, 78, 2, 34, 0],
		[6, 79, 3, 35, 1],
		[7, 80, 4, 36, 2]
	],
	[
		[5, 78, 2, 34, 0],
		[6, 79, 3, 35, 1],
		[7, 80, 4, 36, 2]
	],
]
```


#### Observation 1

在 Image Classification 中，实际上并不需要每个 Neuron 读取整个向量，即并不需要像 Fully Connected 那样设置那么多的 Weight 参数。因为图像通常都具备某种特征，基于这些特征形成对应的模式（Pattern），从而每个 Neuron 只需要关注输入的图像（即高维向量）中对应的一部分即可，并不需要为每个维度设置 Weight 参数。

#### Simplification 1: typical setting

在 Network 中，输入图像（即高维向量）的一部分内容，被用作为 Neuron 的输入，这部分内容被称为 Receptive Field，其 Field 的大小被称为 Kernel Size，相邻 Receptive Field 之间的间隔被称为 stride。

一般情况下，建议同一个 Receptive Field 使用一组 Neuron 来进行处理，Kernel Size 设置为 $3*3$，并且同时包含所有的颜色通道 Channel。相邻的 Receptive Field 之间，建议保留一定的重叠部分，stride 设置为 1 或 2。当 Receptive Field 超出图像的范围，建议使用 Padding 等技术进行补值。

#### Observation 2

同样的 Pattern，可能出现在图像的不同区域中。

#### Simplification 2: parameter sharing

可以尝试在 Neuron 之间共享 Weight 参数，以识别相同的 Pattern，而这些被共享的 Weight 参数也被称为 Filter，同时 Filter 也被用来进行卷积（convolve）计算，用于输出特征图的像素值，而整个卷积层（Convolutional Layer）的输出内容便是特征图（Feature Map）。


#### Observation 3

针对图像进行二次采样（subsampling）并不会改变图像所描述的对象。

#### Simplification 3: Convolutional Layers + Pooling

Pooling（池化）就是一种二次采样的操作，通常紧接着 Convolutional Layer 之后，用于减少 Convolutional Layer 生成的特征图的维度，在一定程度上降低计算量并提取关键特征。

常见的池化操作包括最大池化（Max Pooling）和平均池化（Average Pooling）。

#### The whole CNN

Image Vector -> Convolution -> Pooling -> Convolution -> Pooling -> Flatten -> Fully Connected Layers。


### Self-attention

在神经网络 Model 中，Self-attention （自注意力）技术通常用于 Transformer 模型中的编码器和解码器层。

在 Transformer 模型的编码器层中，Self-attention 被称为自编码器的注意力机制。它用于对输入序列中不同位置之间的依赖关系和重要性进行建模。Self-attention 使得模型可以在序列内部进行自我关注，从而更好地理解序列数据的语义和上下文。

因此，在神经网络模型中，Self-attention 通常被作为一种特殊的层级存在于 Transformer 模型的编码器和解码器层中。它在序列建模和自然语言处理任务中发挥着重要的作用，提供了全局建模和上下文理解的能力。

Self-attention 技术还常用于 Bert 模型中。

#### Sophisticated Input

输入内容变得越来越复杂：
1. 从单个 vector，到多个 vector
2. 从多个等长的 vector，到多个变长的 vector

##### Vector set as input

如：在文本中，组成句子的每个单词的长度不一定相同，将单词编码为向量后，则可以使用一组向量来表示文本。

单词（Word）编码为 Vector 的常用方法：
1. One-hot Encoding
2. Word Embedding

如：一段声音信号，其实也可以表示为一组向量。将一段范围（Window）的声音信号截取后，对应的声音信号，则也可以编码为一个向量，通常这个向量被称为 Frame，而 Window 的长度通常取 25 Milliseconds。

声音信号的 Frame 编码为 Vector 的常用方法：
1. 400 sample points（16KHz）
2. 39-dim MFCC
3. 80-dim filter bank output

如：一个图，也可以表示为一个向量。社交网络，也可以表示为一个向量。一个分子，也可以表示为一个向量。


##### What is the output

###### Each vector has a label
即针对输入的每个 Vector 都需要有对应的输出。

此类 Model 的常见应用：词性标注（POS tagging）、语音识别等。


###### The whole sequence has a label
即针对输入的一组 Vector 都需要有，只有一个输出。

此类 Model 的常见应用：情感分析（Sentiment analysis）、语音辨认等

###### Sequence to sequence.
The model decides the number of lables itself, i.e. sequence to sequence.
即针对输入的任意数量的 Vector，可以有任意个输出。


#### Sequence Labeling

此类场景即指的是前文提到的 Each vector has a label (a label as output)，对于输入的向量组中，Model 对于每个向量都需要有一一对应的输出。

在此类场景中，如果 Model 在生成每个 Vector 的输出时，还需要依赖于整个组中其他 Vector（即 context），那么 Model 的输入向量将变得很大，而对于在 FCNN 这样的模型中，必然需要设置大量的参数，这样就很容易出现运算量过大，以及 Overfitting 的问题。

而 self-attention 技术，就是专门用于处理这种 Vector 之间存在关联的 Vector Sequence 作为输入的场景。使用 self-attention 技术，可以将输入的 Vector 转换为一个包含整个 sequence context 信息的 Vector。


#### Input and Output

在 Self-attention 对应的 Layer 中，输入的是 n 个向量（$a_{1}\dots a_{n}$）（可以是 hidden layer 的输出），输出的是对应的 n 个向量（$b_{1}\dots b_{n}$），其中每个输出的每个向量 b，都是基于输入的整个向量组 $a_{1}\dots a_{n}$，所得来的。

其中输入向量组中，向量之间的关联性 (relevant) 通常使用 $\alpha$ 来表示，其中 $\alpha$ 常用计算方式有 Dot-product 和 Additive。


#### Multi-head self-attention

#### Positional Encoding

将向量的位置信息也进行编码，融入到 Self-attention Layer 的计算中。


### Self-attention v.s. CNN 

CNN is simplified self-attention.

Self-attention is the complex version CNN.


### Transformer

https://www.youtube.com/watch?v=n9TlOhRjYoc&list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&index=12

Transformer 是一种 **Sequence To Sequence** 的 Neural Network Model，并且是一种通用性很强的 Model，应用场景十分广泛。

Transformer 常见的应用场景有：
1. “语音识别”，如输入一段声音讯号，输出识别出的文字，或者转换为某种特定语言文字。
2. “语音合成”，输入某种语言文字，输出是一段声音讯号。
3. “语言翻译”，如输入一种语言文字，输出另一种语言文字。
4. “问答 QA（Question Answer）”：输入问题，输出回答。
5. “文法剖析（Syntactic Parseing）”：输入一段文字，输出一个文法树状结构（也可以视为用作 Sequence 来表示）。
6. “多标签分类（Multi-label Classification）”：每个 Object 可以同时属于多个 Class。输入一个 Object，输出一个 Class Sequence。



#### Encoder

输入一个向量 Sequence，输出一个向量 Sequence。

#### Decoder

输入 Encoder 生成的 Vector Sequence，输出 Model 最终结果。

在 Transformer 模型中，解码器（Decoder） 是负责生成输出序列的部分，通常用于生成目标语言的翻译、生成文章、回答问题等任务。解码器在语言生成任务中起到关键作用，它将编码器生成的上下文信息和之前生成的部分序列，逐步生成下一个单词或标记。

解码器的工作方式类似于生成式的循环神经网络（RNN），但是由于 Transformer 的注意力机制，它可以更好地处理长距离依赖和上下文关系。解码器通常包括多层的自注意力层和前馈神经网络层。 


## Self-supervised Learning

在目前深度学习发展的情况下，对于监督学习，虽然数据很容易获取，但是在数据处理阶段的标注成本，却是非常昂贵的，故而希望使用更少的标注样本就能够训练一个泛化能力很好的模型，故而诞生了 Self-supervised Learning 这类无监督学习方法。

在机器学习工程的模型训练阶段，通常会先采用 Self-supervised Learning 方法和大量的未标注数据来训练一个基础模型，这个阶段也被称为预训练（Pre-train）阶段，而这个阶段训练的模型会具有强大的泛化能力，通过这种方法一方面快速提升了模型的整体性能，另一方面也可以节省在数据处理阶段中大量的数据标注成本。

在预训练阶段结束之后，通常还会紧跟着其他的训练任务，来针对预训练生成的模型，进一步进行训练。

### Supervised vs. Self-supervised

在有监督学习（Supervised Learning）中，训练数据集中的每个样本都有一个与之对应的标签或目标值。算法的目标是学习从输入数据到输出标签的映射关系。具体来说，有监督学习的训练目标是试图找到某组参数，使得损失函数的值最小，即最小化实际输出与目标输出之间的误差。


自监督学习（Self-Supervised Learning）是一种无监督学习（Unsupervised Learning）的模型训练方法，自监督学习的关键思想是，其训练目标不基于标注标签，而是基于数据本身的内在结构，可极大利用现实中易获取的无标签数据进行知识学习。如，通过将输入数据中的某些部分进行遮挡，并将其内容视为“伪标签”，然后训练模型来预测这些伪标签。


有监督学习（Supervised Learning）的优点：
- 有监督学习通常可以获得更好的性能，因为它可以直接利用标签信息。
- 有监督学习的目标函数更加明确，因此可以更容易地进行优化。
- 有监督学习的理论基础更加完善，因此可以更好地理解模型的行为。

有监督学习（Supervised Learning）的缺点：
- 有监督学习需要大量的标签数据，这在某些情况下可能非常昂贵或困难。
- 有监督学习对标签数据的质量要求很高，如果标签数据不准确，可能会导致模型的性能下降。

自监督学习（Self-Supervised Learning）的优点：
- 自监督学习不需要大量的标签数据，因此可以应用于更广泛的任务。
- 自监督学习可以学习到更通用的特征，因为它不依赖于特定的任务。
- 自监督学习可以更有效地利用数据，因为它可以从数据中提取更多的信息。

自监督学习（Self-Supervised Learning）的缺点：
- 自监督学习的目标函数通常不如有监督学习的目标函数明确，因此可能难以进行优化，通常后续还会搭配有监督学习（Supervised Learning）方法来训练模型。
- 自监督学习的理论基础不如有监督学习的理论基础完善，因此可能难以理解模型的行为。

### BERT 

BERT（Bidirectional Encoder Representations from Transformers）是使用自监督学习方法进行预训练的。BERT 的预训练过程涉及两个自监督学习任务：Masked token prediction 和 Next Sentence Prediction。

可以简单的将 BERT 的作用理解为“做填空题”。
#### Masked token prediction

在这个任务中，输入文本中的某些单词会被随机遮盖，然后模型需要预测这些被遮盖的单词。这个任务鼓励模型从上下文中理解单词的语义，以及预测被遮盖单词的身份。

#### Next Sentence Prediction

在这个任务中，模型接收两个句子作为输入，然后预测第二个句子是否是第一个句子的下一个句子。这个任务有助于模型学习句子之间的关系和上下文。

#### BERT的应用场景

- 情感分析（Sentiment analysis）：输入 Sequence，输出 Class。
- 词性标注（POS tagging）：输入 Sequence，输出 Sequence。
- 自然语言推理（Natural Language Inference）：输入 Two Sequence，输出 Class。
- 基于内容的问答（Extraction-based QA）：输入 Sequence，输出 Sequence。
### GPT

GPT (Generative Pre-trained Transformer) 是一种生成式模型，主要用于文本生成。GPT 主要以单向的方式，从左到右逐词生成文本，因此相比于 BERT 在生成文本时通常不具备对整个上下文的全局理解。
#### Predict next token

GPT 的预训练任务是自回归式（auto-regressive）地预测句子中的下一个单词（predict next token）。
#### GPT的应用场景

- 文本生成
- 文章摘要
- 文本翻译
- 文本分析
- 对话问答

### Auto encoder

Auto encoder 虽然早于自监督学习（Self-supervised Learning）概念出现，但也可以看做是一种在预训练的自监督学习方法，因为这种训练方法，同样可以使用未标注的数据来训练模型。
#### Aoto encoder Framework

Auto encoder 中存在 Encoder 和 Decoder 两个 Neural Network，其中 Encoder 的作用是将输入的数据转换为向量，而 Decoder 的作用是将输入的向量转换为对应输出的数据，Auto encoder 的训练目标是使得输入和输出尽量接近。

Auto encoder 的输入可以是 image、audio、text 等。
#### Aoto encoder 的应用场景

语音转换
文本摘要
异常检测


## Adversarial Attack

在机器学习领域，Adversarial Attack（对抗攻击）是指有意地向机器学习模型输入某种扰动，以使模型的输出产生误导或错误的结果。这种扰动通常是微小的、人类难以察觉的，但对于机器学习模型来说却足够影响其性能。Adversarial Attack 的目标是欺骗或混淆模型，使其做出错误的预测或分类。

Adversarial Attack 可以应用于各种机器学习任务，包括图像分类、文本分类、语音识别等。攻击者通常会使用某些算法和技术来生成对抗样本，这些对抗样本与原始输入数据非常相似，但具有引导模型产生错误输出的性质。攻击者可以针对特定的模型进行对抗攻击，也可以采用白盒攻击（攻击者知道模型的结构和参数）或黑盒攻击（攻击者只知道模型的输入和输出，但不知道内部结构）。

Adversarial Attack 可以用于机器学习模型的安全性和鲁棒性的评估。即使模型在常规数据上表现良好，仍然可能在对抗性示例上表现很差。为了提高模型的鲁棒性，研究人员不断努力开发防御方法，以抵御 Adversarial Attack。

### White Box

攻击者知道模型的结构和参数。
#### FGSM

Fast Gradient Sign Method（FGSM）是一种常见的对抗攻击方法，用于攻击神经网络模型。它是一种快速生成对抗示例的方法，通过微小的扰动来欺骗模型，使其在输入数据上产生错误的分类结果。

FGSM 的基本思想是在输入数据上添加一个微小的扰动，该扰动是沿着损失函数的梯度方向（或梯度的符号方向）生成的。这个扰动被设计为足够小，以便人类眼睛难以察觉，但足以使模型产生错误的预测。

### Black Box

攻击者只知道模型的输入和输出，但不知道内部结构。

### Passive Defense

Image Smoothing
Image Compression
Generator
Randomization

### Proactive Defense

Adversarial Training


## Explainable Machine Learning

### Why we need Explainable ML

工程伦理问题不可忽视，如自动驾驶、医疗诊断、银行风险评估等领域，机器学习模型的输出必须是可解释的。
### Explainable/Interpretable VS Powerful

一味追求 Interpretable 而忽视 Powerful，就类似于偏执地在固定的路灯下找处于遗失在黑暗中的钥匙。而一味地追求 Powerful ，而忽视 Interpretable，那么模型就无法在工程伦理要求严格的场景中使用。

### Local explanation and global explanation

Local explanation：指定模型的输入和输出，解释对应的输入产生对应输出的原因，如：为什么你认为这个照片是猫。

Global explanation：指定模型的输出，解释什么样的输入会产生对应的输出，如：你认为什么样的照片是猫。

### Saliency Map

在图像识别领域，可以通过绘制 Saliency Map 可以用于描述输入图像中影响 Model 输出内容的权重分布。 


## Domain Adaptation

在机器学习领域，传统的机器学习方法，一般都假设模型的训练数据集和测试数据集服从相同的概率分布，但在实际应用中，这种假设往往不成立，可能两类数据集的输入数据的概率分布不同，也可能两类数据集的输出数据的概率分布不同，这些情况都被称为 Domain Shift。

Domain Adaptation（领域适应）是指在不同数据分布的数据集上进行机器学习的技术，主要用于解决这类问题。


## Reinforcement Learning

强化学习（Reinforcement Learning）是机器学习领域的一个分支，类比于 Supervised Learning、Unsupervised Learning、Self-supervised Learning。

训练方式主要是在代理（Agent）在与环境（Environment）的交互中，通过试错的方式学习如何在当前状态（State）下做出一系列决策（Action），以最大化奖励（Reward）的过程。

当发现获取标注数据很困难，通过人工标注也无法获取的时候，就可以考虑使用 RL (Reinforcement Learning)。



## 参考链接
1. [李宏毅-【機器學習2021】(中文版)](https://www.youtube.com/playlist?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J)