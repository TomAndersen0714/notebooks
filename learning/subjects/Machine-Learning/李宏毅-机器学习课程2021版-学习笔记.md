# 李宏毅-机器学习课程 2021 版-学习笔记


## Machine Learning

Machine Learning ≈ 确定某个特殊的Function（函数），其具有特定的输入和输出

### Function的类别

**Regression**
Regression: 输入一个序列，输出一个标量（Scalar）

**Classification**
Classification：输入类别（Class）或选项（Option），输出正确的选项或类别

Alpha Go，就是一种Classification，其中class就是棋盘的位置

**Structure Learning**
Structure Learning：输出的内容是结构化的，如图片、文档等


### 确定Function的步骤

#### Define Model

**第一步**：确定带有未知参数的Function，即**确定Model**

基于Domain Knowledge，推测出带参Function的函数式，即推测Model。如一元二次函数、三元四次函数等，线性Model，或者多项式回归、神经网络等非线性Model

其中已知的先验数据集中，**feature**代表数据的数值，**label**则代表数据的分类

#### Define Loss Function

**第二步**：基于Training Data和Model，确定**Loss Function**

Loss Function是基于Training Data，构建出的函数，用于判断Model参数的好坏

Loss Function其输入是Model参数的一组数值，输出的标量，即**误差error或loss**，代表着Model参数的好坏

Loss Function中常常会基于MAE（mean absolute error）、MSE（mean square error）等方式，来计算误差error

以Loss Function的一组输入参数为超平面（hyperplane）坐标，可以是任意（维度）数量的，以error的值作为此超平面的法向量的长度，进而构成的多维空间中的曲面，被称为**误差曲面（error surface）**

#### Optimization

**第三步**：最优化**Optimization**，即**确定最优Parameter**

最优Parameter：使得Loss值最小的Parameter。

每次更新Parameter的过程，被称为update。

Gradient Descent（梯度下降）方法：
随机选取一个起始点，然后通过计算Loss Function中误差曲面（error surface）的微分（即二维的斜率），以便朝着Loss下降最快的方向调整参数，进而逐渐确定Loss最小值。

机器学习过程中，需要自己手动设置的参数，如learning rate等，被称为**Hyperparameter**。
PS：learning rate控制的是每次Parameter在update时的其变化的跨度。



## Deep Learning

深度学习是一种以人工神经网络为基础的机器学习方法，属于机器学习的一个分支，一个具体的实现方法。

其中人工神经网络就是机器学习中的一种Model，神经网络中的每个神经元（Neuron）可以看做是一个函数（Function），一组神经元组成一个层次（Layer），而多个层次共同组成一个神经网络（Neural Network）。

深度学习中的深度（Deep），代表着神经网络模型中的层数较多，即网络中包含多个隐含层的情况。

Function => Neuron

Neurons => Layer

Layers => Neural Network

Deep = Many hidden layers（隐含层）

每个隐含层的输出，是下一个隐含层的输入


## Model常见问题及诊断技巧

先检查Training Data Loss，如果Training Data Loss很大，则可能是Model Bias，或者是Optimization的问题。

如果Training Data Loss很小，则检查Testing Data Loss。

如果Testing Data Loss很大，则可能是Overfitting，或者Mismatch的问题。

反之，如果Testing Data Loss和Testing Data Loss都很小，则代表Model很好。


### Model Bias

如果Training Loss结果太大，则可能是Model Bias的问题。

通常情况下，是因为Model过于简单，导致拟合效果太差，预测结果与真实结果相差太大。

常见的解决方案是重新设计Model，采取更加复杂的Model，如：增加更多的Layer等。



### Optimization Issue

可以通过对比模型，来诊断是否是Optimization环节的问题：层次更多的Model，但是Loss却大于层次更少的Model，可以推断是Optimization的问题。

1. 使用层次少的网络构建Model，或者其他能够容易进行optimization的Model，用于后续对照
2. 使用层次多的网络构建Model，如果在相同Training Data的前提下，Deeper Network Model相比于层次少的Model的Loss更高，则可以推断是Optimization的问题

常用解决方案：
1. 更换Optimization方法，以选取确定更好的Parameter


### Overfitting

在机器学习中，Overfitting（过拟合）是指模型在训练集上表现得很好，但在新数据（测试集或实际应用中的数据）上表现较差的现象。当模型过度拟合训练数据时，它在训练数据上学习到了训练集的细节和噪声，导致在未见过的数据上泛化能力较差。

Overfitting 通常发生在模型具有较高的复杂性或参数数量时，使得模型过于灵活以适应训练数据中的细微变化。这种过度拟合的行为导致模型对训练集中的噪声和异常值过度敏感，而忽略了数据背后的真实模式和普遍规律。

可以通过对比 Training Data 和 Testing Data 下的 Loss，如果 Training Data 下的 Loss 结果，远小于 Testing Data，则可以推断是 Overfitting 的问题。

常用解决方案：
1. 增大Training Data
	1. Data augmentation，如图片放大缩小、左右反转等
	2. 采集新数据
2. 降低Model的灵活度和复杂度
	1. Less parameter, share parameters
	2. Less features
	3. Early stopping
	4. Regularization
	5. Dropout

Training Data的Loss一般会随着Model复杂度的提升，而不断减小，但Testing Data的Loss会先随着Model复杂度的提升而减小，当突破某阈值之后，便会随着Model复杂度的提升而增加，进而出现Overfitting的问题，即Testing Loss要远大于Training Loss。


### Mismatch

Mismatch指的是Training Set和Testing Set在分布上的区别太大，进而使得Testing Loss数值明显大于Training Loss。


### 如何选出最好的Model

Cross Validation

将Training Set分成Training Set和Validation Set，使用Training Set训练Model完成之后，可以使用Validation Set去验证Model，相当于提前验证Model。

避免最后在应用Testing Set时结果不如意。


## Training Batch

Training Set被切分为的子集被称为Batch，而一次Batch的训练过程就是一次update过程，将所有Batch训练过一次的过程被称为一个epoch。

Batch优化方法指的是，将Batch作为Training的基本单位，每次都会计算Loss，以及进行参数的update。

### Small Batch vs Large Batch

1. update速度：在串行运算的情况下，Small Batch更快；在并行运算的情况下，Small Batch和Large Batch差不多。
2. epoch耗时：Small Batch耗时更多，Large Batch耗时更少。
3. Gradient分布：Small Batch下Gradient分布更随机，而Large Batch更集中。
4. Optimization效果：在Optimization阶段，Small Batch优于Large Batch
5. Generalization(Testing)效果：在Testing阶段，即使用新数据测试时，Small Batch训练结果的表现，要优于Large Batch


## Optimization Issue诊断技巧

### 常见的issue

Optimization阶段，即便不断尝试使用新的参数进行训练，但模型的Loss依旧很高，且不怎么变化，那就很可能是遇见了critical point问题，即Gradient值接近于0的情况。

### critical point

critical point问题主要可以分为，local minima和saddle point两大类。

通过计算Hessian矩阵的结果，并结合其eigenvalue，便可以判断当前遇到的critical point问题到底是local minima，还是saddle point的问题。

local minima，相比于saddle point，实际上并没有那么常见。


## Optimizer

在机器学习中，Optimizer（优化器）是指，在Optimization阶段，用于优化模型参数的算法或方法。机器学习的目标通常是通过最小化（或最大化）一个损失函数值来找到最优的模型参数，而优化器则是负责调整模型参数以实现损失函数最小化（或最大化）的工具。

### Momentum Optimizer

Momentum是一种常用的优化算法（Optimizer），用于在Optimization过程中，**调整Gradient的下降方向**，加速Gradient Descent方法的收敛速度，并且可以减少出现Saddle Point和Local Minima问题的可能。

Momentum算法中，Parameter的update方向，不再一味的基于通用Gradient Descent算法的计算结果，而是在此基础上，引入了之前历史的update方向，来作为新方向的计算基础，这种新算法常被称为“Gradient Descent With Momentum”。

### Learning  Rate Optimizer

#### Adaptive Learning  Rate

一般情况下，训练Model时，如果遇到update的次数不断增多，但Loss变化不大的情况时，很容易让人联想到是不是遇见了Critical Point问题，但实际上也有可能是Learning  Rate这种Hyperparameter的值设置不当导致的，即可能Parameter的update step太小（**Gradient的下降速度太慢**），亦或是太大（**Gradient的下降速度太快**）。

理论上，为了保证Optimization的质量，通常需要根据不同的Parameter组，来设置不同的Learning  Rate，因此动态的Learning  Rate就很有必要。

##### Adagrad

Adagrad (Adaptive Gradient)通过计算Gradient的Root Mean Square值，来确定下一次update过程的Learning  Rate参数。

使用这种方式，当Gradient数值较大时，则会降低Learning Rate，反之则会增大Learning Rate，故可以确保稳定的step size进行parameter update。


##### RMS Prop

在Adagrad的Root Mean Square计算公式中，给Gradient增加了可自定义的权重，即Gradient不再是个个平等，而是有轻重之分。

##### Adam

Adam（Adaptive Moment Estimation），即RMS Prop + Momentum。


#### Learning Rate Scheduling

Learning Rate Scheduling，即基于时间来设置不同的Learning Rate。

##### Learning Rate Decay

在Learning Rate Decay方法中，Learning Rate值随着训练时间的增加，而不断衰减，此算法可以使得Loss在前期保持一个较高的衰减速率，而到了训练后期，可以平滑地收敛到某一水平，而不会产生较大的震荡。

##### Warm Up

在Warm Up中，Learning Rate会先变大，后变小，在训练BERT模型时，常常需要使用的Warm Up方法。


## Classification
https://www.youtube.com/watch?v=O2VkP8dJ5FE&list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&index=7


## Batch Normalization
https://www.youtube.com/watch?v=BABPWOkSbLE&list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&index=8


## Neural Network (NN)
https://www.youtube.com/watch?v=OP5HcXJg2Aw&list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&index=9


### Fully Connected Neural Network (FCNN)

FCNN 是一种基本的神经网络模型，由多个全连接层 (Fully Connected Layer) 组成。每个全连接层中的每个神经元都与前一层的所有神经元相连，每个连接都有相应的权重。这意味着每个神经元接收来自前一层所有神经元的输入，并生成输出。

FCNN 中的每层全连接层 (Fully Connected Layer) 中的每一个 Neuron 针对输入的高维向量中每一个维度的分量都需要有 Weight 参数，如一层 Network 有 1000 个 Neuron，而输入的向量维度为 $100*100*3$，则 Model 一共有 $100*100*3*1000=3*10^7$ 个 Weight 参数。

FCNN 在图像识别、文本分类、语音识别等各种机器学习任务中都被广泛应用。它是一种通用的模型，能够学习和表示复杂的非线性关系。

值得注意的是，FCNN 是一种基础的神经网络模型，其仅由全连接层组成，缺乏一些特定的结构，如卷积层和池化层。对于特定的任务和数据类型，可以根据需要对 FCNN 进行调整或与其他类型的层结合使用，以构建更复杂和强大的模型。

### Convolutional Neural Network (CNN)

CNN (Convolutional Neural Network) Model 主要用于图像分类任务，用于将输入的图像分为不同的类别，属于监督和深度学习的一种，是专门用于影像处理的 Neural Network。

#### Image Classification

在 Image Classification 场景中 CNN 的输入输出都是向量。

输出向量的 Dimention 长度决定了模型能够识别的对象的种类，Model 的输出向量和真实答案向量之间的 Cross entropy 越小，则代表模型识别的精度越高。

大于二维的矩阵，被称为张量（Tensor），而每一张图片，一般都是通过三维的 Tensor 来进行存储（即三维数组），这三个维度分别是长度、宽度、通道（channel），而其中的每一个数值，都代表某个二维位置上的某个通道的颜色数值大小，即这个三维立方体中某个位置上存储的颜色数值大小，而元素的角标，则代表其在对应维度的位置。在图像分类的 Model 中，一般输入 Network 的向量，是由张量中各个分量拼接而成的高维向量。

如：
```json
[
	[
		[5, 78, 2, 34, 0],
		[6, 79, 3, 35, 1],
		[7, 80, 4, 36, 2]
	],
	[
		[5, 78, 2, 34, 0],
		[6, 79, 3, 35, 1],
		[7, 80, 4, 36, 2]
	],
	[
		[5, 78, 2, 34, 0],
		[6, 79, 3, 35, 1],
		[7, 80, 4, 36, 2]
	],
]
```


#### Observation 1

在 Image Classification 中，实际上并不需要每个 Neuron 读取整个向量，即并不需要像 Fully Connected 那样设置那么多的 Weight 参数。因为图像通常都具备某种特征，基于这些特征形成对应的模式（Pattern），从而每个 Neuron 只需要关注输入的图像（即高维向量）中对应的一部分即可，并不需要为每个维度设置 Weight 参数。

#### Simplification 1: typical setting

在 Network 中，输入图像（即高维向量）的一部分内容，被用作为 Neuron 的输入，这部分内容被称为 Receptive Field，其 Field 的大小被称为 Kernel Size，相邻 Receptive Field 之间的间隔被称为 stride。

一般情况下，建议同一个 Receptive Field 使用一组 Neuron 来进行处理，Kernel Size 设置为 $3*3$，并且同时包含所有的颜色通道 Channel。相邻的 Receptive Field 之间，建议保留一定的重叠部分，stride 设置为 1 或 2。当 Receptive Field 超出图像的范围，建议使用 Padding 等技术进行补值。

#### Observation 2

同样的 Pattern，可能出现在图像的不同区域中。

#### Simplification 2: parameter sharing

可以尝试在 Neuron 之间共享 Weight 参数，以识别相同的 Pattern，而这些被共享的 Weight 参数也被称为 Filter，同时 Filter 也被用来进行卷积（convolve）计算，用于输出特征图的像素值，而整个卷积层（Convolutional Layer）的输出内容便是特征图（Feature Map）。


#### Observation 3

针对图像进行二次采样（subsampling）并不会改变图像所描述的对象。

#### Simplification 3: Convolutional Layers + Pooling

Pooling（池化）就是一种二次采样的操作，通常紧接着 Convolutional Layer 之后，用于减少 Convolutional Layer 生成的特征图的维度，在一定程度上降低计算量并提取关键特征。

常见的池化操作包括最大池化（Max Pooling）和平均池化（Average Pooling）。

#### The whole CNN

Image Vector -> Convolution -> Pooling -> Convolution -> Pooling -> Flatten -> Fully Connected Layers。


### Self-attention

在神经网络 Model 中，Self-attention （自注意力）技术通常用于 Transformer 模型中的编码器和解码器层。

在 Transformer 模型的编码器层中，Self-attention 被称为自编码器的注意力机制。它用于对输入序列中不同位置之间的依赖关系和重要性进行建模。Self-attention 使得模型可以在序列内部进行自我关注，从而更好地理解序列数据的语义和上下文。

因此，在神经网络模型中，Self-attention 通常被作为一种特殊的层级存在于 Transformer 模型的编码器和解码器层中。它在序列建模和自然语言处理任务中发挥着重要的作用，提供了全局建模和上下文理解的能力。

Self-attention 技术还常用于 Bert 模型中。

#### Sophisticated Input

输入内容变得越来越复杂：
1. 从单个 vector，到多个 vector
2. 从多个等长的 vector，到多个变长的 vector

##### Vector set as input

如：在文本中，组成句子的每个单词的长度不一定相同，将单词编码为向量后，则可以使用一组向量来表示文本。

单词（Word）编码为 Vector 的常用方法：
1. One-hot Encoding
2. Word Embedding

如：一段声音信号，其实也可以表示为一组向量。将一段范围（Window）的声音信号截取后，对应的声音信号，则也可以编码为一个向量，通常这个向量被称为 Frame，而 Window 的长度通常取 25 Milliseconds。

声音信号的 Frame 编码为 Vector 的常用方法：
1. 400 sample points（16KHz）
2. 39-dim MFCC
3. 80-dim filter bank output

如：一个图，也可以表示为一个向量。社交网络，也可以表示为一个向量。一个分子，也可以表示为一个向量。


##### What is the output

Each vector has a label (a label as output)
即针对输入的每个 Vector 都需要有对应的输出。

此类 Model 的常见应用：词性标注（POS tagging）、语音识别等。


The whole sequence has a label (a label as output)
即针对输入的一组 Vector 都需要有，只有一个输出。

此类 Model 的常见应用：情感分析（Sentiment analysis）、语音辨认等


The model decides the number of lables itself, i.e. sequence to sequence.
即针对输入的任意数量的 Vector，可以有任意个输出。


#### Sequence Labeling

此类场景即指的是前文提到的 Each vector has a label (a label as output)，对于输入的向量组中，Model 对于每个向量都需要有一一对应的输出。

在此类场景中，如果 Model 在生成每个 Vector 的输出时，还需要依赖于整个组中其他 Vector（即 context），那么 Model 的输入向量将变得很大，而对于在 FCNN 这样的模型中，必然需要设置大量的参数，这样就很容易出现运算量过大，以及 Overfitting 的问题。

而 self-attention 技术，就是专门用于处理这种 Vector 之间存在关联的 Vector Sequence 作为输入的场景。使用 self-attention 技术，可以将输入的 Vector 转换为一个包含整个 sequence context 信息的 Vector。


#### Input and Output

在 Self-attention 对应的 Layer 中，输入的是 n 个向量（$a_{1}\dots a_{n}$）（可以是 hidden layer 的输出），输出的是对应的 n 个向量（$b_{1}\dots b_{n}$），其中每个输出的每个向量 b，都是基于输入的整个向量组 $a_{1}\dots a_{n}$，所得来的。

其中输入向量组中，向量之间的关联性 (relevant) 通常使用 $\alpha$ 来表示，其中 $\alpha$ 常用计算方式有 Dot-product 和 Additive。


#### Multi-head self-attention

#### Positional Encoding

将向量的位置信息也进行编码，融入到 Self-attention Layer 的计算中。


### Self-attention v.s. CNN 

CNN is simplified self-attention.

Self-attention is the complex version CNN.


### Transformer

https://www.youtube.com/watch?v=n9TlOhRjYoc&list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&index=12

## 参考链接
1. [李宏毅-【機器學習2021】(中文版)](https://www.youtube.com/playlist?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J)