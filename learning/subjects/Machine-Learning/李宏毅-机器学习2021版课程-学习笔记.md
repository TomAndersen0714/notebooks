# 李宏毅-机器学习2021版课程-学习笔记


## Machine Learning

Machine Learning ≈ 确定某个特殊的Function（函数），其具有特定的输入和输出

### Function的类别

**Regression**
Regression: 输入一个序列，输出一个标量（Scalar）

**Classification**
Classification：输入类别（Class）或选项（Option），输出正确的选项或类别

Alpha Go，就是一种Classification，其中class就是棋盘的位置

**Structure Learning**
Structure Learning：输出的内容是结构化的，如图片、文档等


### 确定Function的步骤

#### Define Model

**第一步**：确定带有未知参数的Function，即**确定Model**

基于Domain Knowledge，推测出带参Function的函数式，即推测Model。如一元二次函数、三元四次函数等，线性Model，或者多项式回归、神经网络等非线性Model

其中已知的先验数据集中，**feature**代表数据的数值，**label**则代表数据的分类

#### Define Loss Function

**第二步**：基于Training Data和Model，确定**Loss Function**

Loss Function是基于Training Data，构建出的函数，用于判断Model参数的好坏

Loss Function其输入是Model参数的一组数值，输出的标量，即**误差error或loss**，代表着Model参数的好坏

Loss Function中常常会基于MAE（mean absolute error）、MSE（mean square error）等方式，来计算误差error

以Loss Function的一组输入参数为超平面（hyperplane）坐标，可以是任意（维度）数量的，以error的值作为此超平面的法向量的长度，进而构成的多维空间中的曲面，被称为**误差曲面（error surface）**

#### Optimization

**第三步**：最优化**Optimization**，即**确定最优Parameter**

最优Parameter：使得Loss值最小的Parameter。

每次更新Parameter的过程，被称为update。

Gradient Descent（梯度下降）方法：
随机选取一个起始点，然后通过计算Loss Function中误差曲面（error surface）的微分（即二维的斜率），以便朝着Loss下降最快的方向调整参数，进而逐渐确定Loss最小值。

机器学习过程中，需要自己手动设置的参数，如learning rate等，被称为**Hyperparameter**。
PS：learning rate控制的是每次Parameter在update时的其变化的跨度。



## Deep Learning

深度学习是一种以人工神经网络为基础的机器学习方法，属于机器学习的一个分支，一个具体的实现方法。

其中人工神经网络就是机器学习中的一种Model，神经网络中的每个神经元（Neuron）可以看做是一个函数（Function），一组神经元组成一个层次（Layer），而多个层次共同组成一个神经网络（Neural Network）。

深度学习中的深度（Deep），代表着神经网络模型中的层数较多，即网络中包含多个隐含层的情况。

Function => Neuron

Neurons => Layer

Layers => Neural Network

Deep = Many hidden layers（隐含层）

每个隐含层的输出，是下一个隐含层的输入


## Model常见问题及诊断技巧

先检查Training Data Loss，如果Training Data Loss很大，则可能是Model Bias，或者是Optimization的问题。

如果Training Data Loss很小，则检查Testing Data Loss。

如果Testing Data Loss很大，则可能是Overfitting，或者Mismatch的问题。

反之，如果Testing Data Loss和Testing Data Loss都很小，则代表Model很好。


### Model Bias

如果Training Loss结果太大，则可能是Model Bias的问题。

通常情况下，是因为Model过于简单，导致拟合效果太差，预测结果与真实结果相差太大。

常见的解决方案是重新设计Model，采取更加复杂的Model，如：增加更多的Layer等。



### Optimization Issue

可以通过对比模型，来诊断是否是Optimization环节的问题：层次更多的Model，但是Loss却大于层次更少的Model，可以推断是Optimization的问题。

1. 使用层次少的网络构建Model，或者其他能够容易进行optimization的Model，用于后续对照
2. 使用层次多的网络构建Model，如果在相同Training Data的前提下，Deeper Network Model相比于层次少的Model的Loss更高，则可以推断是Optimization的问题

常用解决方案：
1. 更换Optimization方法，以选取确定更好的Parameter


### Overfitting

可以通过对比Training Data和Testing Data下，如果Loss的Training Data下的计算结果，远小于Testing Data，则可以推断是Overfitting的问题。

常用解决方案：
1. 增大Training Data
	1. Data augmentation，如图片放大缩小、左右反转等
	2. 采集新数据
2. 降低Model的灵活度和复杂度
	1. Less parameter, share parameters
	2. Less features
	3. Early stopping
	4. Regularization
	5. Dropout

Training Data的Loss一般会随着Model复杂度的提升，而不断减小，但Testing Data的Loss会先随着Model复杂度的提升而减小，当突破某阈值之后，便会随着Model复杂度的提升而增加，进而出现Overfitting的问题，即Testing Loss要远大于Training Loss。


### Mismatch

Mismatch指的是Training Set和Testing Set在分布上的区别太大，进而使得Testing Loss数值明显大于Training Loss。


### 如何选出最好的Model

Cross Validation

将Training Set分成Training Set和Validation Set，使用Training Set训练Model完成之后，可以使用Validation Set去验证Model，相当于提前验证Model。

避免最后在应用Testing Set时结果不如意。


## Training Batch

Training Set被切分为的子集被称为Batch，而一次Batch的训练过程就是一次update过程，将所有Batch训练过一次的过程被称为一个epoch。

Batch优化方法指的是，将Batch作为Training的基本单位，每次都会计算Loss，以及进行参数的update。

### Small Batch vs Large Batch

1. update速度：在串行运算的情况下，Small Batch更快；在并行运算的情况下，Small Batch和Large Batch差不多。
2. epoch耗时：Small Batch耗时更多，Large Batch耗时更少。
3. Gradient分布：Small Batch下Gradient分布更随机，而Large Batch更集中。
4. Optimization效果：在Optimization阶段，Small Batch优于Large Batch
5. Generalization(Testing)效果：在Testing阶段，即使用新数据测试时，Small Batch训练结果的表现，要优于Large Batch


## Optimization Issue诊断技巧

### 常见的issue

Optimization阶段，即便不断尝试使用新的参数进行训练，但模型的Loss依旧很高，且不怎么变化，那就很可能是遇见了critical point问题，即Gradient值接近于0的情况。

### critical point

critical point问题主要可以分为，local minima和saddle point两大类。

通过计算Hessian矩阵的结果，并结合其eigenvalue，便可以判断当前遇到的critical point问题到底是local minima，还是saddle point的问题。

local minima，相比于saddle point，实际上并没有那么常见。


## Optimizer

在机器学习中，Optimizer（优化器）是指，在Optimization阶段，用于优化模型参数的算法或方法。机器学习的目标通常是通过最小化（或最大化）一个损失函数值来找到最优的模型参数，而优化器则是负责调整模型参数以实现损失函数最小化（或最大化）的工具。

### Momentum Optimizer

Momentum是一种常用的优化算法（Optimizer），用于在Optimization过程中，**调整Gradient的下降方向**，加速Gradient Descent方法的收敛速度，并且可以减少出现Saddle Point和Local Minima问题的可能。

Momentum算法中，Parameter的update方向，不再一味的基于通用Gradient Descent算法的计算结果，而是在此基础上，引入了之前历史的update方向，来作为新方向的计算基础，这种新算法常被称为“Gradient Descent With Momentum”。

### Learning  Rate Optimizer

#### Adaptive Learning  Rate

一般情况下，训练Model时，如果遇到update的次数不断增多，但Loss变化不大的情况时，很容易让人联想到是不是遇见了Critical Point问题，但实际上也有可能是Learning  Rate这种Hyperparameter的值设置不当导致的，即可能Parameter的update step太小（**Gradient的下降速度太慢**），亦或是太大（**Gradient的下降速度太快**）。

理论上，为了保证Optimization的质量，通常需要根据不同的Parameter组，来设置不同的Learning  Rate，因此动态的Learning  Rate就很有必要。

##### Adagrad

Adagrad (Adaptive Gradient)通过计算Gradient的Root Mean Square值，来确定下一次update过程的Learning  Rate参数。

使用这种方式，当Gradient数值较大时，则会降低Learning Rate，反之则会增大Learning Rate，故可以确保稳定的step size进行parameter update。


##### RMS Prop

在Adagrad的Root Mean Square计算公式中，给Gradient增加了可自定义的权重，即Gradient不再是个个平等，而是有轻重之分。

##### Adam

Adam（Adaptive Moment Estimation），即RMS Prop + Momentum。


#### Learning Rate Scheduling

Learning Rate Scheduling，即基于时间来设置不同的Learning Rate。

##### Learning Rate Decay

在Learning Rate Decay方法中，Learning Rate值随着训练时间的增加，而不断衰减，此算法可以使得Loss在前期保持一个较高的衰减速率，而到了训练后期，可以平滑地收敛到某一水平，而不会产生较大的震荡。

##### Warm Up

在Warm Up中，Learning Rate会先变大，后变小，在训练BERT模型时，常常需要使用的Warm Up方法。


## Classification
https://www.youtube.com/watch?v=O2VkP8dJ5FE&list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&index=7

## Batch Normalization
https://www.youtube.com/watch?v=BABPWOkSbLE&list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&index=8


## Convolutional Neural Network (CNN)
https://www.youtube.com/watch?v=OP5HcXJg2Aw&list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&index=9


### Image Classification

在 Image Classification 中 CNN 的输入输出都是向量。

输出向量的 Dimention 长度决定了模型能够识别的对象的种类，Model 的输出向量和真实答案向量之间的 Cross entropy 越小，则代表模型识别的精度越高。

大于二维的矩阵，被称为张量（Tensor），而一张图片，则是三维的 Tensor，这三个维度分别是长度、宽度、通道（channel）。

## 参考链接
1. [李宏毅-【機器學習2021】(中文版)](https://www.youtube.com/playlist?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J)