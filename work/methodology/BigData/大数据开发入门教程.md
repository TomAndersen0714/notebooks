# 数据开发入门教程

## 大数据开发流程


## 入门技术栈

Python3：[Python3 教程 | 菜鸟教程](https://www.runoob.com/python3/python3-tutorial.html)

ClickHouse SQL：[ClickHouse SQL官方文档](https://clickhouse.com/docs/en/sql-reference/statements/)，[ClickHouse SQL官方文档（中文版）](https://clickhouse.com/docs/zh/sql-reference/statements/#)

Airflow：[Airflow官方文档](https://airflow.apache.org/docs/apache-airflow/stable/index.html)

## 数仓搭建相关代码

airflow1：https://gitlab.xiaoduoai.com/bigdata/airflow/-/tree/master/dev_dags

airflow2：https://gitlab.xiaoduoai.com/bigdata/airflow2/-/tree/master/dev_dags

其中dev_dags路径下所有以xqc_stat为前缀的脚本，皆为明察质检业务线的数仓搭建代码，而其余xqc代码，都为数据集成相关的ETL代码

## 大数据环境

### 开发环境搭建

1. 安装airflow相关python包，支持基于airflow开发

```Shell
python3 -m venv python-venv-airflow1
source python-venv-airflow1/bin/activate
pip3 install -i https://mirrors.aliyun.com/pypi/simple/ apache-airflow==1.10.6


python3 -m venv python-venv-airflow2
source python-venv-airflow2/bin/activate
pip3 install -i https://mirrors.aliyun.com/pypi/simple/ apache-airflow==2.2.5
```

1. 安装airflow内部自研组件到Python airflow安装路径下
   1. airflow1：将https://gitlab.xiaoduoai.com/bigdata/airflow/-/tree/master/contrib/hooks路径下的clickhouse_hook.py、impala_hook、mongo_hook.py等自研airflow组件源码文件，复制到airflow包安装路径的同名文件夹内，如：**`lib/python3.X/site-packages/airflow/contrib/hooks`**
   2. airflow2：将https://gitlab.xiaoduoai.com/bigdata/airflow2/-/tree/master/contrib/hooks路径下的clickhouse_hook.py、impala_hook、mongo_hook.py等自研airflow组件源码文件，复制到airflow包安装路径的同名文件夹内，如：**`lib/python3.X/site-packages/airflow/contrib/hooks`**
   3. PS：自研组件主要作用是封装各个大数据服务组件的API，兼容airflow的同时并提升了部分易用性，通常简单的数仓开发无需直接对接各个数据源的Python API，可直接使用这些airflow自研组件使用对应的大数据服务组件

### 测试和生产环境

[大数据开发、测试、生产环境说明](https://xiaoduoai.feishu.cn/docs/doccn97RmqinKtcTwYoEUvliXdf) 

## 数据仓库基础概念

1. **事实和维度**：
   1. 在维度建模中，将度量称为**事实**，将环境描述为**维度**，维度是用于分析事实所需要的多样环境。**度量通常为数值型数据**，作为事实逻辑表的事实。**事实表通常是存储业务系统直接产生数据，或者基于某张事实表的统计数据生成的新的更大粒度的统计型事实表**。
   2. **维度**为分析数据服务，是记录所处的环境度量，是我们观察业务的角度，用来反应业务的一类属性。属性的集合构成维度，**维度通常为某个实体对象，即维度表中每一行所描述的对象**。例如，在分析交易过程时，可以通过买家、卖家、商品和时间等维度描述交易发生的环境。
2. **维度属性**：
   1. **维度**通常代表现实中某个存在的实体，而维度表中所包含的表述维度信息的列，被称为**维度属性**。维度属性是查询约束条件、分组和报表标签生成的基本来源，是数据易用性的关键。**每个维度通常具备任意个维度属性**，如地点维度，可以是省市县三个维度属性，也可以是只有邮政编码一个维度属性
3. **退化维度（反范式、反规范化）**：
   1. 事实表相对于维度表通常列数更多、表更宽，行增加速度也更快。通过将维度属性直接冗余存储到对应的关联表中，可以避免大数据量的重复JOIN，同时加快查询速度，这种直接存储到事实表中的维度列被称为**退化维度**，可以是业务系统事实表自带的，也可以是ETL过程中新增的列，而将维度属性直接存储到事实表的操作则被称为**维度退化**。与其他存储在维表中的维度一样，退化维度，通常用来进行事实表的过滤查询、聚合查询等
4. **指标**：
   1. 指标分为原子指标和派生指标。**原子指标**是基于某一业务事件行为下的度量，是业务定义中不可再拆分的指标，是具有明确业务含义的名词，体现明确的业务统计口径和计算逻辑，例如支付金额。**派生指标**，可以理解为对原子指标业务统计范围圈定后，聚合而成的新度量（事实）。
   2. **原子指标**=时间戳+修饰词（非时间粒度）+度量（事实）
   3. **派生指标**=时间周期+修饰词（非时间粒度）+原子指标事实聚合
5. **业务限定**：统计的业务范围，筛选出符合业务规则的记录（类似于SQL中Where子句中的条件，但不包括时间类条件）
6. **统计周期**：统计的时间范围，例如最近一天，最近30天等（类似于SQL中Where或Group      By子句中的时间类条件）
7. **（统计）粒度**：
   1. 统计分析的对象或者视角，定义数据需要汇总的程度，可以理解为聚合运算时的分组条件（类似于SQL中的Group By的对象）。**粒度是维度（属性）的组合**，指明指标统计结果的分组。例如，某个指标是某个卖家每天在某个省份的成交额，粒度就是天、卖家、地区这三个维度的组合。在指定粒度时，需要充分考虑到业务和维度的关系，**统计粒度常常作为派生指标的修饰词存在**。
   2. **统计粒度一旦确定，即同时也就确定了统计表的主键（相当于联合主键），即粒度唯一标识表中的一行记录**。
8. **反规范化、逆规范化**：
   1. 反规范化，是相较于传统关系型数据库的常用的**3NF范式**而言的。当原本具有多层次结构的维度属性，被拆分为一系列维度，而不是单一维度时候，很容易与事实表组合，形成复杂较高的**雪花模型**。大多数联机事务处理系统（OLTP）的底层数据结构在设计时，就会采用这种规范化技术，以大幅减少数据冗余，提升数据更新的效率。**在联系分析处理系统（OLAP）来说，数据通常是稳定的，基本不存在类似于OLTP系统中需要支持数据频繁更新的需求**。
   2. 在OLAP系统中采用雪花模型，用户在统计分析过程中需要执行大量的关联操作，使用复杂度高，同时查询性能差，因此**通常会依托某一主维度，将其他维度作为属性合并到某单个维度中，这种操作被称为反规范化处理**。通过反规范化操作，避免了数据在使用时的频繁重复关联操作，大幅度简化了模型，降低了复杂度，提升了查询效率，且没有丢失任何信息。
   3. 通过逆规范化，可以将3NF中的**雪花模型**，复杂度下降为，形成简单的**星型模型**。
9. **数据立方体**：简单理解，一张数仓的统计表，即为一个数据立方体，各个维度（属性），如时间维度的时分秒、如地理维度的省市区，作为立方体的边，多个维度共同确定一个立方体被划分的子立方体大小，即粒度。而每个子立方体中，存放着各个描述事实的度量值，对于一张统计表而言，就是统计指标。

## 数仓开发基本流程

总的来说，数仓开发和传统的数据库开发流程上区别不大，基本也是三步走，即分析——设计——实现

### 分析

1. 分析阶段，主要采用的是**自顶向下**的思想。
2. 主要内容是**梳理**需求方的指标需求，并将其**拆分**、**翻译**为数仓的标准指标定义的形式，即通常为“时间周期+修饰词（非时间粒度）+原子指标聚合”，并判断指标是否为可加性指标，即同粒度指标相加后，依旧具有类似的实际业务含义，如：按照子账号的粒度聚合后的会话量，多个子账号的会话量相加，依旧具备实际业务含义，但uv类指标直接相加通常无实际含义。
3. 其中需要注意的是，在计算某些比较复杂的指标时，需同时先统计组成此指标的一些前置指标（如：比值类指标，需要先计算其分子分母对应的指标），进行纵向的拆分，这会导致实际开发的指标，会多于需求文档中明面上列出的指标。在数仓开发的分析阶段，一定要将这些指标明确下来，避免设计和工作量预估错误。

### 设计

1. 设计阶段，主要采用的是**自底向上**的思想。
2. 主要内容是进行数仓模型的设计，即统计表的**表结构设计**。数仓中的所有指标统计表，都存在其相应的粒度，设计阶段主要是为了确定分析阶段中梳理出的指标应该存放在哪种粒度的统计表中，本阶段可以利用数据字典来查阅目前已经存在的数据表的粒度。
3. 如果已经存在对应粒度的统计表，则直接通过增加新的指标字段即可；如果不存在对应粒度的统计表，对于可加性指标而言，可以尝试寻找更细粒度的统计表，在其中添加对应的指标，在查询时进行聚合，或者从原始业务表中，自底向上进行构建新的统计表；而对于不可加性指标（如：uv等），则需要直接创建对应粒度的新表，用于存放对应粒度下的指标。
4. 设计阶段完成后，便可以确定，本次需求中，有哪些表结构发生了变动，哪些指标需要入库实现定时触发统计。

### 实现

1. 设计阶段完成之后，便是具体的**代码实现**过程，而其中具体的代码，目前多以**SQL**为主。
2. 与传统的数据库开发类似，数仓开发通常也会包含数据完整性约束规则的开发，以保证数据质量。
3. 开发上线完成后，还需要收集代码实现过程中，所有实际发生过的模型（即表结构和字段）变动信息，并将其更新到数据字典中，便于后续在开发同数据域的需求时，能快速判断是否需要新增或使用现有模型，同时提升数据的可复用性，减少元数据的沟通成本。

## 数仓搭建实践案例

[明察质检质检诊断-大数据技术方案](https://xiaoduoai.feishu.cn/docx/LvMDdkNReo0QNmxIeSxc38mZnUf) 

## 数据字典

[明察质检数据字典（大数据侧）](https://xiaoduoai.feishu.cn/docs/doccnDSC1UBuow5rrVo9dqSq1Ee) 

## 参考链接

1. [构建MaxCompute数据仓库的流程_云原生大数据计算服务 MaxCompute-阿里云帮助中心](https://help.aliyun.com/document_detail/114631.html)