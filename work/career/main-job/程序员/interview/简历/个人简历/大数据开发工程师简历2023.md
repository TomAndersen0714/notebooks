# 大数据开发工程师简历 2023

## 个人信息

性别：男，年龄：26 岁
电话：13409736469
邮箱： 13409736469@163.com
微信：TomAndersen
现居城市：四川省成都市

Github： https://github.com/TomAndersen0714
博客： https://blog.csdn.net/TomAndersen

照片：xxx

意向岗位：大数据开发工程师-数仓方向

## 教育经历

2019.09 - 2021.06，华中科技大学 - 计算机技术，硕士，计算机科学与技术学院
2015.09 - 2019.06，中国地质大学（武汉） - 空间信息与数字技术，本科，计算机科学与技术学院

## 专业技能

1. 了解 TCP/IP 协议族中 TCP、HTTP 等计算机网络基础通信协议
2. 了解 Linux Bash Shell 常用命令，具有 Shell 脚本开发经验
3. 熟悉 Java 编程语言，了解 JVM 基本原理，了解 Collection、JUC 等常用开发模块
4. 熟悉 Python 编程语言，了解 Loguru、SQLAlchemy 等常用开发模块
5. 熟悉 Airflow DAG 任务开发，熟悉 Airflow 自定义组件开发，具有 Airflow 源码开发经验
6. 了解 Pulsar、Kafka 消息队列，具有 Pulsar 相关开发经验
7. 熟悉 CDH、ClickHouse、Airflow 等大数据组件的部署和运维
8. 熟悉 ClickHouse SQL，熟悉 ClickHouse SQL 性能调优方法
9. 了解 HDFS、YARN、Hive、Spark 等大数据组件，了解 MR 开发框架，了解 Hive SQL 性能调优方法
10. 熟悉数仓搭建方法论，熟悉数仓优化常用方法，具有 ClickHouse 数仓搭建实践经验
11. 了解数据治理的基础理论，具有一定的数据治理开发实践经验

## 工作经历

### 20210701~至今

任职公司：晓多科技（成都晓多科技有限公司）
任职部门：大数据平台组
工作岗位：大数据开发工程师

工作内容：
1. 负责特定业务线的所有数据需求开发，其中包括 ETL 任务开发、数仓搭建、数据报表搭建、数据大屏搭建等。
2. 负责组内大数据开源组件运维，保障集群运行稳定。负责数据质量治理工具开发，编写数据治理规范和操作文档。
3. 负责组内大数据开发工具研发，提升组内数据开发能力和开发效率。

获奖情况：
1. 2022 年度奖项-突飞猛进奖-CEM 产品线团队（核心成员）
2. 2021 年度奖项-最佳协作奖（成员）
3. 2021 Q3-创新探索项目奖（成员）

## 职业证书

1. 计算机技术与软件专业技术资格-中级-软件设计师
2. DAMA 数据治理工程师（CDGA）
3. CDA 数据分析师（Level I）
4. 英语 CET-6

## 项目经历

### 电商客服服务质量监控大屏

项目时间：202109~202111

项目介绍：为助力电商企业，帮助提升客服团队的服务质量，本项目基于客户的实时会话消息流，开发搭建了客服接待实时监控数据大屏，实现了客服接待过程中各项数据指标的秒级实时计算、更新和可视化，提供了针对客服接待质检内容的统一在线实时监控、实时告警等能力，成功赋能客户帮助企业大幅提高了各部门客服团队的服务质量。截至目前，已成功为 100+ 企业，1500 +店铺提供了相关服务，满足了客户针对在线接待过程的实时监控需求。

相关组件：Pulsar、xdvector、ClickHouse、Airflow、DataForce 等

负责内容：
1. 数据集成：对接上游业务系统，通过企业自研的流数据处理平台 xdvector 开发 Pulsar 流实时消费程序，对客服实时会话日志数据进行实时 ETL 后写入 ClickHouse Buffer 表中，保证数据秒级预处理和写入，进而构建实时数仓 ODS 和 DWD 层，同时通过 Airflow 定期调度数据同步任务将各企业组织架构、员工信息等维度数据到 ClickHouse Replicated 表中，构建实时数仓的 DIM 层。
2. 实时数仓搭建：采用 ClickHouse ReplacingReplicatedMergeTree 表存储会话数据，通过数据副本保证高可用的同时，支持数据按照指定会话 ID 进行后台数据合并，搭配 Merge On Read 机制以实现数据的近实时更新，最后通过 ClickHouse Distribued 表提供数据的实时数据查询和数据分析服务。
3. 数据可视化：基于企业自研的低代码可视化平台 DataForce，以 ClickHouse 作为数据源，通过前端 EChars 组件，实现客服接待相关数据指标的实时计算和可视化。

项目难点及解决思路：
1. 基于 ClickHouse 的近实时数据更新方案的设计与实现：通过技术调研，以及线下测试环境的性能对比测试，最终确定技术方案的实现路径，并将相关技术文档分享组内，实现技术沉淀。
2. 历史明细数据实时查询时会因时间范围过大而出现性能问题：在产品层面，通过和产品达成一致，及时约束数据更新操作的时间范围；在技术层面，针对历史数据创建任务进行定期合并 DWD 层明细数据，同时定期统计指标写入 DWS 层，加速查询。

项目总结与反思：
1. 数据需求分析阶段，需要做好数据探查，预估数据日增量，以及数据查询性能，提前和业务侧确定产品方案细则，避免产品功能发布后出现变动和违约，影响客户使用体验。
2. 项目完结时，要做好相关文档沉淀，技术方案的调研和设计，需要及时同步到组内知识库，必要时进行宣传和共享，与组内成员共同交流和进步。

### 电商客服服务质量数据报表

项目时间：202202~202303

项目介绍：为提高电商企业的客服服务质量，优化电商企业数据分析师的工作效率，本项目基于上游客服接待质检平台的质检结果，通过构建离线数仓 T+1 离线统计 100+客服服务质量关键数据指标，实现了不同模块的质检数据报表自动化产出，截止目前，已成功为 400+企业、3000+店铺客服团队的服务质量评估、绩效考核、客服能力评估和培训、客服数据分析报告等工作内容提供了数据支撑和流程提效。

相关组件：MongoDB、xdvector、ClickHouse、Airflow、DataForce 等

负责内容：
1. 数据集成：对接上游客服质检业务系统，进行数据离线 Airflow ETL 任务开发，定期同步企业组织架构、员工信息等配置数据，作为离线数仓的公共维度层 DIM ，T+1 增量同步 MongoDB 中的客服会话数据，经过反规范化后处理后与历史数据合并更新，作为搭建离线数仓的数据接入层 ODS 和明细数据层 DWD 。
2. 离线数仓搭建：基于 DWD 层中客服会话质检结果等明细事实表，以及 DIM 层相关的商家配置维度表，将不同的数据指标进行拆解细分，构建指标矩阵，将相同的数据域和指标粒度进行汇聚，实现指标聚合统计以构建数据汇总层 DWS 。最后按照相同的应用主题、功能模块，将 DWS 层中的指标济进行汇聚，提升数据的易用性，形成数据应用层 ADS。
3. 数据可视化：基于内部自研的低代码可视化平台 Dataforce，搭建数据报表的可视化看板，按不同维度展示对应的数据指标，并支持条件筛选查询。

项目难点及解决思路：
1. 业务方面，产品迭代迅速，而自身是首次加入该团队，需要快速学习业务和产品相关知识：首先通过查阅业务线对应产品知识库中的文档、视频等相关资料，了解产品产生的背景、目标、功能等，然后主动和产品经理进行沟通交流，查漏补缺。
2. 技术方面，数据项目历史代码参与人员较多、各模块代码和表结构参差不齐，历史包袱重，开发成本高：首先需要调研和摸清代码和数据库现状，创建文档通过图表等方式描述代码和数据现状，然后按照优先级罗列待办事项，最后按照不同优先级将待办事项排期依次解决。

项目总结与反思：
1. 在解决历史包袱过程中，可以使用四象限法辅助排期，并按不同的事项主题进行汇聚，然后逐渐拆分事项，并排期到日常迭代周期中进行解决。按事项的主题进行划分，是为了避免做的事情不够聚焦，导致工作汇报时，无法体现该工作的产出和价值；而拆分排期，是为了尽量减小这些重要不紧急事项对于主线需求排期的影响，避免与主线排期冲突，导致重要项目延期。

### 数据计算存储资源和数据质量治理

项目时间：202303~202306

项目介绍：随着内部的数据相关业务和数据量的不断增长，大数据集群的数据量以及负载的数据查询请求数量也不断增多，为了降低服务器资源开销和数据运维成本，实现大数据集群的降本增效，本项目通过开发数据治理基础工具，以及针对计算和存储资源开销、数据质量进行治理，最终实现了高负载查询、无效数据表、数据质量问题报告的自动生成，提高数据运维效率，降低了服务器资源开销和运维成本。

相关组件：Airflow、ClickHouse、CDH、Impala、Kudu、HDFS 等

负责内容：
1. 数据治理工具开发：开发 Airflow 插件，支持定时任务调用插件发送消息到飞书群。修改 Airflow 源码，将 Airflow 告警信息接入飞书群，实现线上定时任务的实时监控和预警信息采集统一。
2. 数据计算资源治理：开发 Airflow 定时任务，定期查询 ClickHouse query-log 数据，同时自动生成高负载 SQL 统计报告并发送告警群，为性能调优提供数据支撑和优化方向，进而降低 ClickHouse 集群的 CPU 和内存资源开销。
3. 数据存储治理：与下游业务线数据产品负责人协商敲定数据生命周期信息，通过 Airflow 定时任务，读取表的生命周期配置，及时清理过期历史数据；通过定期查询 query-log，统计并自动报告各数据表访问次数，通过与业务方沟通对齐后，将近期内无访问记录的数据表和任务下线。
4. 数据质量治理：基于业务线指标统计规则，开发相应的数据测试用例，通过 Airflow 定时执行数据测试用例，实现数据质量的在线监控，尽早发现和处理数据质量问题，降低下游开发成本。

项目难点及解决思路：
1. 跨组工作事项难以推进：通过提前确定好待办事项，然后沟通预约相应部门负责人，确定该项目的参与人员与工作事项大致估时和排期，确保足够的资源分配；通过定期小型会议的形式，及时同步和对齐项目进度，遇到无法解决的卡点和难点时，需要及时向上反应，报备风险，必要时进行事态升级，保障项目整体进度。

项目总结与反思：
1. 项目过程中需要做好过程指标采集，为最后项目成果汇报的量化部分提供数据支持：任何项目在最后都需要进行项目成功总结和汇报，项目开发过程中不能只关注项目进度，还需要注重项目过程指标的采集，避免某些数据因为生命周期较短（如服务器日志数据），错过采集时间后影响后续成果量化。

### 电商买家咨询热点统计报表（废弃）

项目时间：202306~202309

项目介绍：（一期）为了支撑和提效电商平台商家针对卖家咨询的问题进行的数据分析行为，利用分词工具，对客服的聊天数据进行分词，统计词频，抽取买家咨询热点，为电商商家提供数据分析方向。（二期）基于算法模型针对客服聊天会话数据的分类结果，搭建每日用户画像，统计中各个标签下的 UV、PV 等指标，同时支持用户下钻至原始会话，搭配大模型 AI 能力，增强和提效商家的数据分析能力。

相关组件：jiba、hanlp、LTP、LAC、Airflow、MongoDB、ElasticSearch、ClickHouse、DataForce 等

负责内容：
1. 技术调研：对比各个开源的中文文本分词工具，使用测试集进行评测
2. 数据集成：基于 Airflow 定时使用开源分词工具将 ClickHouse 中的会话聊天数据进行切割，并将处理后的数据重新写入 ClickHouse
3. 离线数仓搭建：基于 ClickHouse 中通过分词处理后的聊天数据，搭建离线数仓，统计生产词频、咨询人数等指标
4. 数据可视化：基于内部的低代码可视化平台 Dataforce，通过词云等可视化组件，展示各个维度下的词频统计等指标。

项目难点及解决思路：

项目总结与反思：